---
title: "Movie Recommendation System"
author: "Alexis Koffi"
date: "07/02/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this project, I demonstrate a film recommender system capable of recommending movies to users based on a rating scale. We will use the following libraries:

```{r loading-libs, message=FALSE, error=FALSE, echo=TRUE}
library(tidyverse)
library(caret)
library(ggplot2)
```

##### Data Loading

We develop the algorithm using the *edx* set, and have a final test with the *validation* set. These datasets are extracted with the project's code available at [https://courses.edx.org/](https://courses.edx.org/courses/course-v1:HarvardX+PH125.9x+2T2018/courseware/dd9a048b16ca477a8f0aaf1d888f0734/e8800e37aa444297a3a2f35bf84ce452/?child=last).

```{r data-preparation, message=FALSE, error=FALSE, echo=FALSE}

# Create test and validation sets

#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
     semi_join(edx, by = "movieId") %>%
     semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

# Methods

##### Data summary

*edx* and *validation* are both in tidy format

```{r data-head-view, message=FALSE, error=FALSE, echo=TRUE}
# Subset of edx and validation data
head(edx[, 1:3])
head(validation[, 1:3])
```

Number of unique users that provided ratings and for how many unique movies they provided :

```{r n-users-ratings, message=FALSE, error=FALSE, echo=TRUE}
edx %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))
```

The first thing we notice is that some movies get rated more than others. Here is the distribution :

```{r edx-movies-ratings-distr, message=FALSE, error=FALSE, echo=TRUE}
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Movies")
```

Our second observation is that some users are more active than others at rating movies :

```{r edx-users-ratings-distr, message=FALSE, error=FALSE, echo=FALSE}
edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Users")
```

##### Training and Testing

```{r train-test-set, message=FALSE, error=FALSE, echo=TRUE}
# Define test and train datasets using edx: 80% sample for training, and 20% sample for testing.

set.seed(1)
train_index <- createDataPartition(y = edx$rating, times = 1, p = 0.8, list = FALSE)
train_set <- edx[train_index, ]
temp <- edx[-train_index, ]

# Make sure userId and movieId in test set are also in train set

test_set <- temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Add rows removed from test set back into train set

removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)

rm(temp, removed) # remove temporary datasets
```

For the recommendation systems, we will use three approaches and choose the best model with the lowest RMSE

##### RMSE Calculations

```{r rmse_fun, message=FALSE, error=FALSE, echo=TRUE}
# Root Mean Square Error function
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

In this first model, we predict the same rating for all movies regardless of user :

```{r train-mean, message=FALSE, error=FALSE, echo=TRUE}
# Raw mean of the training dataset
mu_hat <- mean(train_set$rating)
mu_hat
model_1_rmse <- RMSE(test_set$rating, mu_hat)
model_1_rmse
```

As we go along, we will be comparing two others approaches. 
Let’s start by creating a results table with this naive approach :
```{r rmse-result, message=FALSE, error=FALSE, echo=TRUE}
rmse_results <- data_frame(method = "Just the average", RMSE = model_1_rmse)
rmse_results%>%knitr::kable()
```

I consider the movie effects in the second model.

```{r  movie-effect, message=FALSE, error=FALSE, echo=TRUE}
# fit <- lm(rating ~ as.factor(userId), data = movielens)
# the lm() function will be very slow here because there are thousands of bias, each movie gets one
# I use instead the least square estimate :

mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
```

These estimates vary substantially :

```{r movie-avg-qplot, message=FALSE, error=FALSE, echo=TRUE}
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))
```

Let’s see how much the RMSE improves with this second model :

```{r model-2, message=FALSE, error=FALSE, echo=TRUE}
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
```

In the third model, I apply the user effects.
Let’s compute the average rating for user u for those that have rated over 100 movies.
Notice that there is substantial variability across users as well: some users are very cranky and others love every movie.

```{r user-effect, message=FALSE, error=FALSE, echo=TRUE}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")

# User-specific effect model : lm(rating ~ as.factor(movieId) + as.factor(userId))
# We will compute an approximation instead for the reasons described earlier in 2nd model

user_avgs <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

```

We can now construct predictors and see how much the RMSE improves :

```{r model-3, message=FALSE, error=FALSE, echo=TRUE}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_3_rmse ))
```

# Results

##### Models RMSE

The 3rd model has the lowest RMSE and will be used for the final testing of the validation set

```{r rmse-global, message=FALSE, error=FALSE, echo=TRUE}
rmse_results %>% knitr::kable()
```

##### RMSE of the validation set

```{r validation-test, message=FALSE, error=FALSE, echo=TRUE}
## Validation test
# We compute first the user effect for validation set 

user_avgs_validation <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs_validation, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred
model_rmse_validation <- RMSE(predicted_ratings, validation$rating)
model_rmse_validation
```

# Conclusion and discussion

In this project, I have developed and evaluated the naive approach, the movie effects and the user effects for recommending movies. The movie effects and user effects bring big improvements. The dataset provided, also includes the ratings timestamp, and movies genres. To go further, theses variables could be analyzed to aspire to lower the RMSE and develop a better predictive method.

